<workflow-app name='import-collection-to-hbase-via-sf' xmlns="uri:oozie:workflow:0.2">
    <start to='check-table-existence'/>
    <action name='check-table-existence'>
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property> 
            </configuration>
            <main-class>pl.edu.icm.coansys.commons.hbase.HBaseTableUtils</main-class>
            <java-opts></java-opts>
            <arg>true</arg>
            <arg>EXIST</arg> 
            <arg>${outputTableName}</arg>  
            <capture-output/>
        </java>       
        <ok to='go-if-table-exists'/>
        <error to='kill'/>
    </action>
    <decision name="go-if-table-exists">
        <switch>
            <case to="select-import-method">
                ${wf:actionData('check-table-existence')['exit.value']}
            </case>
            <default to="kill"/>
        </switch>
    </decision>
    <decision name="select-import-method">
        <switch>
            <case to="importtsv-put">
                ${importtsvViaPut}
            </case>
            <case to="importtsv-hfile-partifion-file">
                ${!importtsvViaPut}
            </case>
            <default to="kill"/>
        </switch>
    </decision>
    <action name='importtsv-put'>
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
            </prepare>
            <configuration>
                <!-- This is required for new api usage -->
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <!-- General job parameters -->
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <!-- MapReduce Mapper/Reducer parameters -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.importers.io.writers.hbase.DocumentWrapperSequenceFileToHBase$DocumentWrapperToHBasePutMapper</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>0</value>
                </property>
                <!-- MapReduce input/output parameters -->
                <property>
                    <name>mapred.input.dir</name>
                    <value>${collectionDocumentWrapperSequenceFile}</value>
                </property>
                <property>
                    <name>hbase.mapred.outputtable</name>
                    <value>${outputTableName}</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.hbase.mapreduce.TableOutputFormat</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.hbase.io.ImmutableBytesWritable</value>
                </property>                
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.Writable</value>
                </property>
                <property>
                    <name>mapred.child.java.opts</name>
                    <value>${mapredChildJavaOpts}</value>
                </property>
                <property>
                    <name>mapred.map.tasks.speculative.execution</name>
                    <value>false</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks.speculative.execution</name>
                    <value>false</value>
                </property> 
                <property>
                    <name>hbase.client.keyvalue.maxsize</name>
                    <value>${hbaseClientKeyvalueMaxsize}</value>
                </property>
                
            </configuration>
        </map-reduce>
        <ok to='post-import-cleanup'/>
        <error to='kill'/>
    </action>
    <action name='importtsv-hfile-partifion-file'>
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property> 
            </configuration>
            <main-class>pl.edu.icm.coansys.commons.hbase.IncrementalLoadConfigurator</main-class>
            <java-opts></java-opts>
            <arg>${outputTableName}</arg> 
            <capture-output/>
        </java>       
        <ok to='importtsv-hfile'/>
        <error to='kill'/>
    </action>
    <action name='importtsv-hfile'>
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${importtsvBulkOutput}"/>
            </prepare>
            <configuration>
                <!-- This is required for new api usage -->
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>   
                <!-- General job parameters -->
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>     
                <!-- MapReduce Mapper/Reducer parameters -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.importers.io.writers.hbase.DocumentWrapperSequenceFileToHBase$DocumentWrapperToHBasePutMapper</value>
                </property>
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>org.apache.hadoop.hbase.mapreduce.PutSortReducer</value>
                </property>
                <!-- MapReduce input/output parameters -->
                <property>
                    <name>mapred.input.dir</name>
                    <value>${collectionDocumentWrapperSequenceFile}</value>
                </property>
                <property>
                    <name>hbase.mapred.outputtable</name>
                    <value>${outputTableName}</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.hbase.mapreduce.HFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.hbase.io.ImmutableBytesWritable</value>
                </property>                
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.hbase.KeyValue</value>
                </property>
                <property>
                    <name>mapred.mapoutput.key.class</name>
                    <value>org.apache.hadoop.hbase.io.ImmutableBytesWritable</value>
                </property>                
                <property>
                    <name>mapred.mapoutput.value.class</name>
                    <value>org.apache.hadoop.hbase.client.Put</value>
                </property>
                <property>
                    <name>mapreduce.partitioner.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${importtsvBulkOutput}</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>${wf:actionData('importtsv-hfile-partifion-file')['mapred.reduce.tasks']}</value>
                </property>
                <property>
                    <name>mapred.create.symlink</name>
                    <value>yes</value>
                </property>
                <property>
                    <name>mapred.child.java.opts</name>
                    <value>${mapredChildJavaOpts}</value>
                </property>
                <property>
                    <name>mapred.map.tasks.speculative.execution</name>
                    <value>false</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks.speculative.execution</name>
                    <value>false</value>
                </property>
                <property>
                    <name>io.sort.mb</name>
                    <value>${ioSortMb}</value>
                </property>
                <property>
                    <name>io.sort.spill.percent</name>
                    <value>${ioSortSpillPercent}</value>
                </property>
                <property>
                    <name>io.sort.record.percent</name>
                    <value>${ioSortRecordPercent}</value>
                </property>
            </configuration>
            <file>${wf:actionData('importtsv-hfile-partifion-file')['mapred.cache.file.symlinked']}</file>
        </map-reduce>
        <ok to='chmod-hfile'/>
        <error to='kill'/>
    </action>
    <action name="chmod-hfile">
        <fs>
            <chmod path='${importtsvBulkOutput}' permissions='777' dir-files='true'/>
        </fs>
        <ok to="completebulkload"/>
        <error to="kill"/>
    </action>
    <action name='completebulkload'>
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property> 
            </configuration>
            <main-class>org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles</main-class>
            <java-opts></java-opts>
            <arg>${importtsvBulkOutput}</arg>
            <arg>${outputTableName}</arg>
        </java>       
        <ok to='post-completebulkload-cleanup'/>
        <error to='kill'/>
    </action> 
    <action name="post-completebulkload-cleanup">
        <fs>
            <delete path="${wf:actionData('importtsv-hfile-partifion-file')['mapred.cache.file']}"/> 
        </fs>
        <ok to="post-import-cleanup"/>
        <error to="kill"/>
    </action>
    <action name="post-import-cleanup">
        <fs>
            <delete path='${workflowPath}'/> 
        </fs>
        <ok to="end"/>
        <error to="kill"/>
    </action>
    <kill name='kill'>
        <message>Workflow failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name='end'/>
</workflow-app>
