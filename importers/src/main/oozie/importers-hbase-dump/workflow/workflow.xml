<workflow-app name='hbase-dump' xmlns="uri:oozie:workflow:0.2">
    <start to='check-table-existence'/>
    <action name='check-table-existence'>
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property> 
            </configuration>
            <main-class>pl.edu.icm.coansys.commons.hbase.HBaseTableUtils</main-class>
            <java-opts></java-opts>
            <arg>true</arg>
            <arg>EXIST</arg> 
            <arg>${bwndataTableName}</arg>  
            <capture-output/>
        </java>       
        <ok to='go-if-table-exists'/>
        <error to='kill'/>
    </action>
    <decision name="go-if-table-exists">
        <switch>
            <case to="dump-table">
                ${wf:actionData('check-table-existence')['exit.value']}
            </case>
            <default to="kill"/>
        </switch>
    </decision>
    <action name='dump-table'>
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${bwndataDumpOutputDir}"/>
            </prepare>
            <configuration>
                <!-- This is required for new api usage -->
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <!-- General job parameters -->
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <property>
                    <name>hbase.zookeeper.quorum</name>
                    <value>${hbaseZookeeperQuorum}</value>
                </property>
                <!-- MapReduce Mapper/Reducer parameters -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.importers.io.writers.hbase.HBaseToDocumentProtoSequenceFile$RowToDocumentProtoMapper</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>0</value>
                </property>
                <!-- MapReduce input/output parameters -->
                <property>
                    <name>hbase.mapreduce.inputtable</name>
                    <value>${bwndataTableName}</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${bwndataDumpOutputDir}</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.hbase.mapreduce.TableInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>                
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>
                <property>
                    <name>mapred.map.tasks.speculative.execution</name>
                    <value>false</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks.speculative.execution</name>
                    <value>false</value>
                </property> 
                <property>
                    <name>hbase.client.scanner.caching</name>
                    <value>100</value> 
                </property>
		<property>
		    <name>mapred.child.java.opts</name>
		    <value>${hbaseDump_mapredChildJavaOpts}</value>
		</property>
                <!-- Multiple output -->
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.mproto.key</name>
                    <value>org.apache.hadoop.io.BytesWritable</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.mproto.value</name>
                    <value>org.apache.hadoop.io.BytesWritable</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.dproto.value</name>
                    <value>org.apache.hadoop.io.BytesWritable</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.dproto.key</name>
                    <value>org.apache.hadoop.io.BytesWritable</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs</name>
                    <value>mproto cproto dproto</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.mproto.format</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.dproto.format</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.cproto.format</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.cproto.key</name>
                    <value>org.apache.hadoop.io.BytesWritable</value> 
                </property>
                <property>
                    <name>mapreduce.multipleoutputs.namedOutput.cproto.value</name>
                    <value>org.apache.hadoop.io.BytesWritable</value> 
                </property>
            </configuration>
        </map-reduce>
        <ok to='end'/>
        <error to='kill'/>
    </action>
    <kill name='kill'>
        <message>Workflow failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name='end'/>
</workflow-app>
