package pl.edu.icm.coansys.disambiguation.author.pig;

import java.io.IOException;
import java.util.Arrays;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;

import org.apache.pig.EvalFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DefaultDataBag;
import org.apache.pig.data.DefaultTuple;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.slf4j.LoggerFactory;

import pl.edu.icm.coansys.commons.java.StackTraceExtractor;
import pl.edu.icm.coansys.disambiguation.author.features.disambiguators.DisambiguatorFactory;
import pl.edu.icm.coansys.disambiguation.features.Disambiguator;
import pl.edu.icm.coansys.disambiguation.features.FeatureInfo;
import pl.edu.icm.coansys.disambiguation.author.benchmark.Timer;

public class AproximateAND extends EvalFunc<DataBag> {

	private float threshold;
	private PigDisambiguator[] features;
	private FeatureInfo[] featureInfos;
	private float sim[][];
	private Tuple datain[];
	private int N;
	//TODO: change lists to arrays where possible
    private static final org.slf4j.Logger logger = LoggerFactory.getLogger(AproximateAND.class);
    //benchmark staff
    private Timer timer;
    private int calculatedSimCounter; //using for statistics generated by timer
    private int timerPlayId = 0;
    private int finalClusterNumber = 0;
    
	public AproximateAND(String threshold, String featureDescription){
		this.threshold = Float.parseFloat(threshold);
		
		List<FeatureInfo> FIwithEmpties 
			= FeatureInfo.parseFeatureInfoString(featureDescription);
		List<FeatureInfo> FIFinall = new LinkedList<FeatureInfo>();
		List<PigDisambiguator> FeaturesFinall = new LinkedList<PigDisambiguator>();
		
        DisambiguatorFactory ff = new DisambiguatorFactory();
        Disambiguator d;
        
        //separate features which are fully described and able to use
        for ( FeatureInfo fi : FIwithEmpties ){
        	if ( fi.getDisambiguatorName().equals("") ) continue;
        	if ( fi.getFeatureExtractorName().equals("") ) continue;
        	d = ff.create(fi);
        	if ( d == null ) continue;
        	FIFinall.add( fi );
        	FeaturesFinall.add( new PigDisambiguator( d ) );
        }
        
		this.featureInfos = FIFinall.toArray( new FeatureInfo[ FIFinall.size() ] );
        this.features = 
        		FeaturesFinall.toArray( new PigDisambiguator[ FIFinall.size() ] );

        timer = new Timer("logs/aproximate.stat");
        (new Thread( timer )).start();
        timer.addMonit("id","contribs","clusters","calculated sims","time");
        timer.addMonit("S:", "=suma(B3:B1000000)", "=suma(C3:C1000000)", "=suma(D3:D1000000)","=suma(E3:E1000000)");
	}

	
	/**
	 * @param Tuple (sname:chararray,{(contribId:chararray,contribPos:int,
	 * sname:chararray, metadata:map[{(chararray)}])},count:int)
	 * @see org.apache.pig.EvalFunc#exec(org.apache.pig.data.Tuple)
	 */
	@SuppressWarnings("unchecked")
	@Override
	public DataBag exec( Tuple input ) throws IOException {

		if ( input == null || input.size() == 0 ) return null;
		try {
			//TODO optional:
			//it would be enough to take as argument only map bag with datagroup.
			//In that case this  function would be proof against table changes.
			//This change should be done during generating tables in pig script.

			DataBag contribs = (DataBag) input.get(0);  //taking bag with contribs

			if ( contribs == null || contribs.size() == 0 ) return null;

			//start benchmark
			timer.play();
			calculatedSimCounter = 0;
			timerPlayId++;
			
			Iterator<Tuple> it = contribs.iterator();
			N = (int) contribs.size();

			datain = new DefaultTuple[ N ];

			List< Map<String,Object> > contribsT = 
					new LinkedList< Map<String,Object> > ();

			int k = 0;
			//iterating through bag, dumping bug to Tuple array
			while ( it.hasNext() ) { 
				Tuple t = it.next();
				datain[ k++ ] = t;
				contribsT.add( (Map<String, Object>) t.get(3) ); //map with features
				//TODO: change map to list (disambiguators are created one by one 
				//as feature does, so both of them will be iterated in the same order).
				//change map to databag in pig script? (memory for keys with 
				//strings of extractors' names would be saved)
			}

			//sim[][] init
			sim = new float[ N ][];
			for ( int i = 1; i < N; i++ ) {
				sim[i] = new float[i];

				for ( int j = 0; j < i; j++ ) {
					sim[i][j] = threshold;
                }
			}

			//sim[][] calculating
			calculateAffinityAndClustering( contribsT );

			//clusterList[ cluster_id ] = { contribs in cluster.. }
			int[][] clustersArray = splitIntoClusters();
			
	        //stopping timer for current play (not thread)
	        //this action will add some information to timer monit
			for ( int[] cluster : clustersArray ) 
				finalClusterNumber += ( cluster.length == 0 )?0:1;		
	        timer.stop( timerPlayId, N, finalClusterNumber, calculatedSimCounter );
	        
	        //bag: Tuple with (Object with (String (UUID), bag: { Tuple with ( String (contrib ID) ) } ) )
	        return createResultingTuples( clustersArray );

		}catch(Exception e){
			// Throwing an exception would cause the task to fail.
			logger.error("Caught exception processing input row:\n" 
						+ StackTraceExtractor.getStackTrace(e));
				return null;
		}
	}

	//Find & Union
	//USE find() to call cluster id for each contributors from clusterAssicuiations tab!
	private int clusterAssociations[], clusterSize[];

	//finding cluster associations
	// o( log*( n ) ) ~ o( 1 )
	private int find( int a ) {
		if ( clusterAssociations[a] == a ) return a;
		int fa = find( clusterAssociations[a] );
		//Correcting cluster associations to the top one during traversing along the path.
		//Path is splited and straight connection with representative of union (cluster id) is made.
		clusterAssociations[a] = fa;
		return fa;
	}

	//cluster representatives union
	//o( 1 )
	private boolean union( int a, int b ) {
		int fa = find( a );
		int fb = find( b );

		if ( fa == fb ) return false;
		//choosing bigger cluster, union representatives in one cluster
		if (clusterSize[fa] <= clusterSize[fb]) {
			clusterSize[fb] += clusterSize[fa];
			clusterSize[fa] = 0; //because cluster with id 'fa' does not exist anymore
			clusterAssociations[fa] = fb;
		}
		else {
			clusterSize[fa] += clusterSize[fb];
			clusterSize[fb] = 0; 
			clusterAssociations[fb] = fa;
		}

		return true;
	}

	private void calculateAffinityAndClustering( List< Map<String,Object> > contribsT ) throws ExecException {
		//Find & Union init:		
		clusterAssociations = new int[N];
		clusterSize = new int[N];

		for ( int i = 0; i < N; i++ ) {
			clusterAssociations[i] = i;
			clusterSize[i] = 1;
		}

		//o( n^2 * features.length )
		//Skipping complexity of find because of its low complexity
		//The heuristic is that o( features.length ) would executed less frequently.
		double partial;
		for ( int i = 1; i < N; i++ ) {
			for ( int j = 0; j < i; j++ ) {

				//if i,j are already in one union, we say they are identical
				//and do not calculate precise similarity value
				if ( find( i ) == find( j ) ) {
					sim[i][j] = Float.POSITIVE_INFINITY;
					continue;
				}
				
				calculatedSimCounter++;
				
				for ( int d = 0; d < features.length; d++ ) {
					//Taking features from each keys (name of extractor = feature name)
					//In contribsT.get(i) there is map we need.
					//From this map (collection of i'th contributor's features)
					//we take Bag with value of given feature.
					//Here we have sure that following Object = DateBag.
					Object oA = contribsT.get(i).get( featureInfos[d].getFeatureExtractorName() );
					Object oB = contribsT.get(j).get( featureInfos[d].getFeatureExtractorName() );
					
					if ( oA == null || oB == null ) continue;
					
					partial = features[d].calculateAffinity( oA, oB );
					
					if ( featureInfos[d].getMaxValue() == 0 ) continue;
					partial = partial / featureInfos[d].getMaxValue() 
							* featureInfos[d].getWeight();
					sim[i][j] += partial;

					//potentially the same contributors
        			if ( sim[i][j] >= 0 ) {
        				//so we union them in one cluster
        				union( i, j );
        				break;
        			}
				}
			}
		}
		
		//features = null;
		//featureInfos = null;
		contribsT = null;
	}

	// o( N )
	private int simIdToClusterId[];
	protected int[][] splitIntoClusters() {
		// cluster[ cluster id ] =  array with contributors' simIds 		
		simIdToClusterId = new int[N];
		int[][] clusters = new int[N][];
		int index[] = new int[N];
		
		for( int i = 0; i < N; i++ ) {
			clusters[i] = new int[ clusterSize[i] ];
			index[i] = 0;
		}
		
		int id;
        for ( int i = 0; i < N; i++ ) {
        	id = find( i );
        	clusters[ id ][ index[id] ] = i;
            simIdToClusterId[ i ] = index[id];
            index[id]++;
        }

		return clusters;
	}

	//o ( N * max_cluster_size )
	protected DataBag createResultingTuples( int[][] clusters ) {
		//IdGenerator idgenerator = new UuIdGenerator();
    	DataBag ret = new DefaultDataBag();
    	DataBag contribDatas;
    	DataBag similarities;
    	
    	//iterating through clusters
    	for ( int[] cluster: clusters ) {
    		//skipping empty clusters
    		if ( cluster == null || cluster.length == 0 ) continue;
    		
        	contribDatas = new DefaultDataBag();
        	similarities = new DefaultDataBag();

        	//iterating through contribs (theirs simId) in cluster
        	for ( int i = 0; i < cluster.length; i++ ) {

        		int sidX = cluster[ i ];

        		//simIdToClusterId[ sidX ] = i;
        		contribDatas.add( datain[ sidX ] );

        		//adding precise calculated similarity values
        		//o( cluster_size )
        		for ( int j = 0; j < i; j++ ) {
        			int sidY = cluster[ j ];

        			if ( sidX <= sidY ||  simIdToClusterId[ sidX ] <= simIdToClusterId[ sidY ] ) {
        				String m = "Trying to write wrong data during create tuple: ";
        				m += ", sidX: " + sidX + ", sidY: " + sidY + ", simIdToClusterId[ sidX ]: " + simIdToClusterId[ sidX ] + ", simIdToClusterId[ sidY ]: " + simIdToClusterId[ sidY ];
        				throw new IllegalArgumentException( m );
        			}

        			if ( sim[ sidX ][ sidY ] != Float.NEGATIVE_INFINITY 
        					&& sim[ sidX ][ sidY ] != Float.POSITIVE_INFINITY ) {
        				Object[] clusterTriple = 
        						new Object[]{ simIdToClusterId[ sidX ], simIdToClusterId[ sidY ], sim[ sidX ][ sidY ] };
        				similarities.add( TupleFactory.getInstance().newTuple( 
        						Arrays.asList( clusterTriple ) ) );
        			}
        		}
        	}

        	Object[] to = new Object[]{ contribDatas, similarities };
	        ret.add(TupleFactory.getInstance().newTuple(Arrays.asList(to)));
        }
 
    	//bag with: { bag with date as in input but ordered by clusters, bag with triple similarities }
    	return ret;
	}
}
