<workflow-app xmlns="uri:oozie:workflow:0.4" name="parent-disambiguation-wf">
	<parameters>
		<property>
			<name>POOL_NAME</name>
			<value>default</value>
		</property>
	</parameters>

	<start to="parent-disambiguation" />
	<action name="parent-disambiguation">
		<map-reduce>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
<!--			<prepare>
				<delete path="${hdfsWorkingDirURI}/intermediate-data" />
			</prepare>-->
			<configuration>
				<property>
					<name>oozie.launcher.mapred.fairscheduler.pool</name>
					<value>${POOL_NAME}</value>
				</property>
				<!-- This is required for new api usage -->
				<property>
					<name>mapred.mapper.new-api</name>
					<value>true</value>
				</property>
				<property>
					<name>mapred.reducer.new-api</name>
					<value>true</value>
				</property>
				<property>
					<name>dfs.client.socket-timeout</name>
					<value>${DFS_SOCKET_CLIENT_TIMEOUT}</value>
				</property>
				<property>
					<name>mapreduce.map.class</name>
					<value>pl.edu.icm.coansys.source.MapDocProtosToSourceBasIds</value>
				</property>
                <property>
					<name>mapreduce.reduce.class</name>
					<value>pl.edu.icm.coansys.source.ReduceDocsWithSameSourceId</value>
				</property>
				<property>
					<name>mapred.map.tasks</name>
					<value>6</value>
				</property>
				<property>
					<name>mapred.reduce.tasks</name>
					<value>6</value>
				</property>
				<property>
					<name>mapred.input.dir</name>
					<value>${hdfsDirInputData}</value>
				</property>
				<property>
					<name>mapred.output.dir</name>
					<value>${hdfsDirOutputData}</value>
				</property>
				<property>
					<name>mapred.job.queue.name</name>
					<value>${queueName}</value>
				</property>
				<property>
					<name>mapreduce.inputformat.class</name>
					<value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat
					</value>
				</property>
				<property>
					<name>mapreduce.outputformat.class</name>
					<value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat
					</value>
				</property>
				<property>
					<name>mapred.mapoutput.key.class</name>
					<value>org.apache.hadoop.io.Text</value>
				</property>
				<property>
					<name>mapred.mapoutput.value.class</name>
					<value>org.apache.hadoop.io.BytesWritable</value>
				</property>
				<property>
					<name>mapred.output.key.class</name>
					<value>org.apache.hadoop.io.Text</value>
				</property>
				<property>
					<name>mapred.output.value.class</name>
					<value>org.apache.hadoop.io.BytesWritable</value>
				</property>
			</configuration>
		</map-reduce>
		<ok to="end" />
		<error to="fail" />
	</action>
	<kill name="fail">
		<message>Workflow failed</message>
	</kill>
	<end name="end" />
</workflow-app>
