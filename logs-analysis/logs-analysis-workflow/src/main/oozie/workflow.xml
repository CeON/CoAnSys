<workflow-app xmlns="uri:oozie:workflow:0.1" name="logs-analysis-wf">
    <start to="count-part" />
    <action name="count-part">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${hdfsWorkingDirURI}/intermediate-data" />
            </prepare>
            <configuration>
                <!-- This is required for new api usage -->
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.logsanalysis.jobs.CountUsagesPart$CounterMap</value>
                </property>
                <property>
                    <name>mapreduce.combine.class</name>
                    <value>pl.edu.icm.coansys.logsanalysis.jobs.CountUsagesPart$CounterReduce</value>
                </property>
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>pl.edu.icm.coansys.logsanalysis.jobs.CountUsagesPart$CounterReduce</value>
                </property>
                <property>
                    <name>mapred.map.tasks</name>
                    <value>6</value>
                </property>
                <property>
                    <name>mapred.input.dir</name>
                    <value>${hdfsDirInputData}</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${hdfsWorkingDirURI}/intermediate-data</value>
                </property>
                <property>
                  <name>mapred.job.queue.name</name>
                  <value>${queueName}</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.mapoutput.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>                
                <property>
                    <name>mapred.mapoutput.value.class</name>
                    <value>org.apache.hadoop.io.LongWritable</value>
                </property>                
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>                
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.LongWritable</value>
                </property>
                <property>
                    <name>USAGE_WEIGHT_CLASS</name>
                    <value>${USAGE_WEIGHT_CLASS}</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="sort-part"/>
        <error to="finalize-error"/>
    </action>
    <action name="sort-part">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${hdfsDirOutputData}" />
            </prepare>            
            <configuration>
                <!-- This is required for new api usage -->
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>                
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.logsanalysis.jobs.SortUsagesPart$SorterMap</value>
                </property>
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>pl.edu.icm.coansys.logsanalysis.jobs.SortUsagesPart$SorterReduce</value>
                </property>
                <property>
                    <name>mapred.map.tasks</name>
                    <value>6</value>
                </property>
                <property>
                    <name>mapred.input.dir</name>
                    <value>${hdfsWorkingDirURI}/intermediate-data</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${hdfsDirOutputData}</value>
                </property>
                <property>
                  <name>mapred.job.queue.name</name>
                  <value>${queueName}</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.mapoutput.key.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapred.mapoutput.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>
                <property>
                    <name>NB_OF_RECORDS</name>
                    <value>${NB_OF_RECORDS}</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="finalize" />
        <error to="finalize-error" />
    </action>
    <action name="finalize">
        <fs>
            <delete path="${hdfsWorkingDirURI}/intermediate-data" />
        </fs>
        <ok to="end" />
        <error to="fail" />
    </action>
    <action name="finalize-error">
        <fs>
            <delete path="${hdfsWorkingDirURI}/intermediate-data" />
        </fs>
        <ok to="fail" />
        <error to="fail" />
    </action>
    <kill name="fail">
        <message>Workflow failed</message>
    </kill>
    <end name="end" />
</workflow-app>
