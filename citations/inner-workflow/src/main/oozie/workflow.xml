<!--
  ~ (C) 2010-2012 ICM UW. All rights reserved.
  -->

<!--

Workflow properties:

 - cit_workDir - workflow working directory
 - jobTracker
 - nameNode
 - queueName
 - cit_inputSrcSeqFile - source file containing citations to be matched in (CitId as Text, $cit_inputSrcFormat) format
 - cit_inputSrcFormat - either BasicMetadata or Raw
 - cit_inputDestSeqFile - documents the citations will be matched against in (DocId as Text, BasicMetadata as Bytes) format
 - cit_outputSeqFile - the matching in (CitId as Text, DocId as Text) format.

-->

<workflow-app xmlns="uri:oozie:workflow:0.1" name="citation-matching-inner-wf">
    <start to="prelude"/>
    <!--<start to="add-heuristic"/>-->
    <action name="prelude">
        <fs>
            <delete path="${cit_workDir}"/>
            <mkdir path="${cit_workDir}"/>
        </fs>
        <ok to="source-entities-type"/>
        <error to="fail"/>
    </action>

    <decision name="source-entities-type">
        <switch>
            <case to="convert-basicmetadata-source-entities">${cit_inputSrcFormat eq "BasicMetadata"}</case>
            <case to="convert-raw-source-entities">${cit_inputSrcFormat eq "Raw"}</case>
            <default to="fail"/>
        </switch>
    </decision>

    <action name="convert-raw-source-entities">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.ReferencesToEntitiesConverter</main-class>
            <arg>-Dmapred.max.split.size=500000</arg>
            <arg>-fs</arg><arg>${nameNode}</arg>
            <arg>-jt</arg><arg>${jobTracker}</arg>
            <arg>${cit_inputSrcSeqFile}</arg>
            <arg>${cit_workDir}/source_entities</arg>
        </java>

        <ok to="convert-destination-entities"/>
        <error to="fail"/>
    </action>

    <action name="convert-basicmetadata-source-entities">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.BasicMetadataToEntitiesConverter</main-class>
            <arg>-fs</arg><arg>${nameNode}</arg>
            <arg>-jt</arg><arg>${jobTracker}</arg>
            <arg>${cit_inputSrcSeqFile}</arg>
            <arg>${cit_workDir}/source_entities</arg>
        </java>

        <ok to="convert-destination-entities"/>
        <error to="fail"/>
    </action>

    <action name="convert-destination-entities">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.BasicMetadataToEntitiesConverter</main-class>
            <arg>-fs</arg><arg>${nameNode}</arg>
            <arg>-jt</arg><arg>${jobTracker}</arg>
            <arg>${cit_inputDestSeqFile}</arg>
            <arg>${cit_workDir}/destination_entities</arg>
        </java>

        <ok to="generate-key-index"/>
        <error to="fail"/>
    </action>

    <action name="generate-key-index">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.IndexBuilder</main-class>
            <arg>-fs</arg><arg>${nameNode}</arg>
            <arg>-jt</arg><arg>${jobTracker}</arg>
            <arg>-key</arg>
            <arg>${cit_workDir}/destination_entities</arg>
            <arg>${cit_workDir}/key_index</arg>
        </java>

        <ok to="generate-author-index"/>
        <error to="fail"/>
    </action>
    <action name="generate-author-index">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.IndexBuilder</main-class>
            <arg>-fs</arg><arg>${nameNode}</arg>
            <arg>-jt</arg><arg>${jobTracker}</arg>
            <arg>-author</arg>
            <arg>${cit_workDir}/destination_entities</arg>
            <arg>${cit_workDir}/author_index</arg>
        </java>

        <ok to="add-heuristic"/>
        <error to="fail"/>
    </action>
    <!--
    <join name="generated-indices" to="do-matching"/>
    -->
    <action name="add-heuristic">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${cit_workDir}/source_entities_heur"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <!-- General job parameters -->
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <!-- MapReduce Mapper/Reducer parameters -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.citations.mappers.HeuristicAdder</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>0</value>
                </property>
                <!-- MapReduce input/output parameters -->
                <property>
                    <name>mapred.input.dir</name>
                    <value>${cit_workDir}/source_entities</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${cit_workDir}/source_entities_heur</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.max.split.size</name>
                    <value>5000000</value>
                </property>
                <property>
                    <name>index.key</name>
                    <value>key_index</value>
                </property>
                <property>
                    <name>index.author</name>
                    <value>author_index</value>
                </property>
            </configuration>
            <file>${cit_workDir}/key_index/data#key-data</file>
            <file>${cit_workDir}/key_index/index#key-index</file>
            <file>${cit_workDir}/author_index/data#author-data</file>
            <file>${cit_workDir}/author_index/index#author-index</file>
        </map-reduce>
        <ok to="assess"/>
        <error to="fail"/>
    </action>

    <action name="assess">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${cit_workDir}/matching"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <!-- General job parameters -->
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <!-- MapReduce Mapper/Reducer parameters -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.citations.mappers.ExactAssesor</value>
                </property>
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>pl.edu.icm.coansys.citations.reducers.BestSelector</value>
                </property>
                <property>
                    <name>mapreduce.combine.class</name>
                    <value>pl.edu.icm.coansys.citations.reducers.BestSelector</value>
                </property>
                <!-- MapReduce input/output parameters -->
                <property>
                    <name>mapred.input.dir</name>
                    <value>${cit_workDir}/source_entities_heur</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${cit_workDir}/matching</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.max.split.size</name>
                    <value>5000000</value>
                </property>
                <property>
                    <name>index.key</name>
                    <value>key_index</value>
                </property>
                <property>
                    <name>index.author</name>
                    <value>author_index</value>
                </property>
            </configuration>
            <file>${cit_workDir}/key_index/data#key-data</file>
            <file>${cit_workDir}/key_index/index#key-index</file>
            <file>${cit_workDir}/author_index/data#author-data</file>
            <file>${cit_workDir}/author_index/index#author-index</file>
        </map-reduce>
        <ok to="end"/>
        <error to="fail"/>
    </action>

    <kill name="fail">
        <message>Workflow failed</message>
    </kill>
    <end name="end"/>
</workflow-app>
