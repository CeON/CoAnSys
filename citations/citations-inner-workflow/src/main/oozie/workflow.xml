<!--
  ~ This file is part of CoAnSys project.
  ~ Copyright (c) 2012-2013 ICM-UW
  ~
  ~ CoAnSys is free software: you can redistribute it and/or modify
  ~ it under the terms of the GNU Affero General Public License as published by
  ~ the Free Software Foundation, either version 3 of the License, or
  ~ (at your option) any later version.
  ~
  ~ CoAnSys is distributed in the hope that it will be useful,
  ~ but WITHOUT ANY WARRANTY; without even the implied warranty of
  ~ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  ~ GNU Affero General Public License for more details.
  ~
  ~ You should have received a copy of the GNU Affero General Public License
  ~ along with CoAnSys. If not, see <http://www.gnu.org/licenses/>.
  -->

<workflow-app xmlns="uri:oozie:workflow:0.4" name="citation-matching-inner-wf">
    <parameters>
        <property>
            <name>cit_workDir</name>
            <description>workflow working directory</description>
        </property>
        <property>
            <name>jobTracker</name>
        </property>
        <property>
            <name>nameNode</name>
        </property>
        <property>
            <name>queueName</name>
            <value>default</value>
        </property>
        <property>
            <name>cit_inputSrcSeqFile</name>
            <description>
                source file containing citations to be matched in (CitId as Text, $cit_inputSrcFormat) format
            </description>
        </property>
        <property>
            <name>cit_inputSrcFormat</name>
            <value>Raw</value>
            <description>either BasicMetadata or Raw</description>
        </property>
        <property>
            <name>cit_genAuthorIdxJavaOpts</name>
            <!-- we need to put something other than an empty string here to get a proper default value -->
            <value>-showversion</value>
            <description>additional Java options for author index building phase</description>
        </property>
        <property>
            <name>cit_heuristic</name>
            <value>Old</value>
        </property>
        <property>
            <name>cit_inputDestSeqFile</name>
            <description>
                documents the citations will be matched against in (DocId as Text, BasicMetadata as Bytes) format
            </description>
        </property>
        <property>
            <name>cit_outputSeqFile</name>
            <description>the matching in (CitId as Text, DocId as Text) format</description>
        </property>
    </parameters>

    <start to="prelude"/>
    <!--<start to="add-heuristic"/>-->
    <action name="prelude">
        <fs>
            <delete path="${cit_workDir}"/>
            <mkdir path="${cit_workDir}"/>
        </fs>
        <ok to="source-entities-type"/>
        <error to="fail"/>
    </action>

    <decision name="source-entities-type">
        <switch>
            <case to="convert-basicmetadata-source-entities">${cit_inputSrcFormat eq "BasicMetadata"}</case>
            <case to="convert-raw-source-entities">${cit_inputSrcFormat eq "Raw"}</case>
            <default to="fail"/>
        </switch>
    </decision>

    <action name="convert-raw-source-entities">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.ReferencesToEntitiesConverter</main-class>
            <arg>-Dmapred.max.split.size=500000</arg>
            <arg>-fs</arg>
            <arg>${nameNode}</arg>
            <arg>-jt</arg>
            <arg>${jobTracker}</arg>
            <arg>${cit_inputSrcSeqFile}</arg>
            <arg>${cit_workDir}/source_entities</arg>
        </java>

        <ok to="convert-destination-entities"/>
        <error to="fail"/>
    </action>

    <action name="convert-basicmetadata-source-entities">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.BasicMetadataToEntitiesConverter</main-class>
            <arg>-fs</arg>
            <arg>${nameNode}</arg>
            <arg>-jt</arg>
            <arg>${jobTracker}</arg>
            <arg>${cit_inputSrcSeqFile}</arg>
            <arg>${cit_workDir}/source_entities</arg>
        </java>

        <ok to="convert-destination-entities"/>
        <error to="fail"/>
    </action>

    <action name="convert-destination-entities">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.BasicMetadataToEntitiesConverter</main-class>
            <arg>-fs</arg>
            <arg>${nameNode}</arg>
            <arg>-jt</arg>
            <arg>${jobTracker}</arg>
            <arg>${cit_inputDestSeqFile}</arg>
            <arg>${cit_workDir}/destination_entities</arg>
        </java>

        <ok to="generate-key-index"/>
        <error to="fail"/>
    </action>

    <action name="generate-key-index">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.IndexBuilder</main-class>
            <arg>-fs</arg>
            <arg>${nameNode}</arg>
            <arg>-jt</arg>
            <arg>${jobTracker}</arg>
            <arg>-key</arg>
            <arg>${cit_workDir}/destination_entities</arg>
            <arg>${cit_workDir}/key_index</arg>
        </java>

        <ok to="heuristic-type"/>
        <error to="fail"/>
    </action>

    <decision name="heuristic-type">
        <switch>
            <case to="generate-author-index">${cit_heuristic eq "Old"}</case>
            <case to="add-new-heuristic">${cit_heuristic eq "New"}</case>
            <default to="fail"/>
        </switch>
    </decision>

    <action name="add-new-heuristic">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.NewHeuristicAdder</main-class>
            <arg>-fs</arg>
            <arg>${nameNode}</arg>
            <arg>-jt</arg>
            <arg>${jobTracker}</arg>
            <arg>${cit_workDir}/source_entities</arg>
            <arg>${cit_workDir}/destination_entities</arg>
            <arg>${cit_workDir}/source_entities_heur</arg>
        </java>

        <ok to="assess"/>
        <error to="fail"/>
    </action>

    <action name="generate-author-index">
        <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <main-class>pl.edu.icm.coansys.citations.jobs.IndexBuilder</main-class>
            <java-opts>${cit_genAuthorIdxJavaOpts}</java-opts>
            <arg>-fs</arg>
            <arg>${nameNode}</arg>
            <arg>-jt</arg>
            <arg>${jobTracker}</arg>
            <arg>-author</arg>
            <arg>${cit_workDir}/destination_entities</arg>
            <arg>${cit_workDir}/author_index</arg>
        </java>

        <ok to="add-heuristic"/>
        <error to="fail"/>
    </action>

    <action name="add-heuristic">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${cit_workDir}/source_entities_heur"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <!-- General job parameters -->
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <!-- MapReduce Mapper/Reducer parameters -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.citations.mappers.HeuristicAdder</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>0</value>
                </property>
                <!-- MapReduce input/output parameters -->
                <property>
                    <name>mapred.input.dir</name>
                    <value>${cit_workDir}/source_entities</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${cit_workDir}/source_entities_heur</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.max.split.size</name>
                    <value>5000000</value>
                </property>
                <property>
                    <name>index.key</name>
                    <value>key_index</value>
                </property>
                <property>
                    <name>index.author</name>
                    <value>author_index</value>
                </property>
            </configuration>
            <file>${cit_workDir}/key_index/data#key-data</file>
            <file>${cit_workDir}/key_index/index#key-index</file>
            <file>${cit_workDir}/author_index/data#author-data</file>
            <file>${cit_workDir}/author_index/index#author-index</file>
        </map-reduce>
        <ok to="assess"/>
        <error to="fail"/>
    </action>

    <action name="assess">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${cit_outputSeqFile}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <!-- General job parameters -->
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <!-- MapReduce Mapper/Reducer parameters -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>pl.edu.icm.coansys.citations.mappers.ExactAssesor</value>
                </property>
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>pl.edu.icm.coansys.citations.reducers.BestSelector</value>
                </property>
                <property>
                    <name>mapreduce.combine.class</name>
                    <value>pl.edu.icm.coansys.citations.reducers.BestSelector</value>
                </property>
                <!-- MapReduce input/output parameters -->
                <property>
                    <name>mapred.input.dir</name>
                    <value>${cit_workDir}/source_entities_heur</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${cit_outputSeqFile}</value>
                </property>
                <property>
                    <name>mapreduce.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.max.split.size</name>
                    <value>5000000</value>
                </property>
                <property>
                    <name>index.key</name>
                    <value>key_index</value>
                </property>
                <property>
                    <name>index.author</name>
                    <value>author_index</value>
                </property>
            </configuration>
            <file>${cit_workDir}/key_index/data#key-data</file>
            <file>${cit_workDir}/key_index/index#key-index</file>
            <file>${cit_workDir}/author_index/data#author-data</file>
            <file>${cit_workDir}/author_index/index#author-index</file>
        </map-reduce>
        <ok to="end"/>
        <error to="fail"/>
    </action>

    <kill name="fail">
        <message>Workflow failed</message>
    </kill>
    <end name="end"/>
</workflow-app>
