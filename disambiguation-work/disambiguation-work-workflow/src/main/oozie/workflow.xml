<workflow-app xmlns="uri:oozie:workflow:0.4" name="disambiguation-work-wf">
    <parameters>
        <property>
            <name>DFS_SOCKET_CLIENT_TIMEOUT</name>
            <value>120000</value>
        </property>
        <property>
            <name>MAPRED_TASK_TIMEOUT</name>
            <value>3600000</value>
        </property>
        <property>
            <name>POOL_NAME</name>
            <value>default</value>
        </property>
        <property>
            <name>INITIAL_MAX_DOCS_SET_SIZE</name>
            <value>1000</value>
        </property>
        <property>
            <name>MAX_DOCS_SET_SIZE_INC</name>
            <value>200</value>
        </property>
        <property>
            <name>MAX_SPLIT_LEVEL</name>
            <value>10</value>
        </property>
    </parameters>
    <start to="disambiguation-work" />
    <action name="disambiguation-work">
		<map-reduce>
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<configuration>
				<!-- This is required for new api usage -->
				<property>
					<name>mapred.mapper.new-api</name>
					<value>true</value>
				</property>
				<property>
					<name>mapred.reducer.new-api</name>
					<value>true</value>
				</property>
				<property>
					<name>mapred.input.dir</name>
					<value>${inputSeqFile}</value>
				</property>
				<property>
					<name>mapred.output.dir</name>
					<value>${outputSeqFile}</value>
				</property>
				<property>
					<name>mapreduce.map.class</name>
					<value>pl.edu.icm.coansys.commons.spring.DiMapper</value>
				</property>
				<property>
					<name>mapreduce.reduce.class</name>
					<value>pl.edu.icm.coansys.commons.spring.DiReducer</value>
				</property>
				<property>
					<name>mapred.map.tasks</name>
					<value>6</value>
				</property>
				<property>
					<name>mapred.reduce.tasks</name>
					<value>8</value>
				</property>
				<property>
					<name>mapred.job.queue.name</name>
					<value>${queueName}</value>
				</property>
				<property>
					<name>mapreduce.inputformat.class</name>
					<value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat
					</value>
				</property>
				<property>
					<name>mapreduce.outputformat.class</name>
					<value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat
					</value>
				</property>
				<property>
					<name>mapred.mapoutput.key.class</name>
					<value>org.apache.hadoop.io.Text</value>
				</property>
				<property>
					<name>mapred.mapoutput.value.class</name>
					<value>org.apache.hadoop.io.BytesWritable</value>
				</property>
				<property>
					<name>mapred.output.key.class</name>
					<value>org.apache.hadoop.io.Text</value>
				</property>
				<property>
					<name>mapred.output.value.class</name>
					<value>org.apache.hadoop.io.Text</value>
				</property>
				<property>
					<name>diMapApplicationContextPath</name>
					<value>spring/applicationContext.xml</value>
				</property>
				<property>
					<name>diMapServiceBeanName</name>
					<value>duplicateWorkDetectMapService</value>
				</property>
				<property>
					<name>diReduceApplicationContextPath</name>
					<value>spring/applicationContext.xml</value>
				</property>
				<property>
					<name>diReduceServiceBeanName</name>
					<value>duplicateWorkDetectReduceService</value>
				</property>
				<property>
					<name>dfs.client.socket-timeout</name>
					<value>${DFS_SOCKET_CLIENT_TIMEOUT}</value>
				</property>
				<property>
					<name>mapred.task.timeout</name>
					<value>${MAPRED_TASK_TIMEOUT}</value>
				</property>
				<property>
					<name>mapred.child.java.opts</name>
					<value>-Xmx4096m</value>
				</property>
				<property>
					<name>mapred.fairscheduler.pool</name>
					<value>${POOL_NAME}</value>
				</property>
			</configuration>
		</map-reduce>
		<ok to="finalize" />
		<error to="finalize-error" />
	</action>

	<kill name="finalize-error">
		<message>Workflow failed</message>
	</kill>
	<end name="finalize" />
</workflow-app>
